<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[【python3爬虫】利用python的单线程和多线程下载PDF电子书]]></title>
    <url>%2F2018%2F12%2F15%2F%E3%80%90python3%E7%88%AC%E8%99%AB%E3%80%91%E5%88%A9%E7%94%A8python%E7%9A%84%E5%8D%95%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8B%E8%BD%BDPDF%E7%94%B5%E5%AD%90%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[环境配置：python3.6 ；win7 本文是参考了https://mp.weixin.qq.com/s/cQRG7Ix0nP4MptlaMqSWvA 。但是发现不能正确运行。所以笔者打算救活这只爬虫。1http://www.allitebooks.com/ 该网站提供了大量免费的编程方面的电子书，可以供大家练手。 爬虫思路1-获取书的页数2-获取书的分页3-获取书的下载链接4-下载5-加入多线程 1-获取书的页数 ，分析网址构造首页 ： http://www.allitebooks.com/第二页：http://www.allitebooks.com/page/2/第三页： http://www.allitebooks.com/page/3/最后一页：http://www.allitebooks.com/page/780/好明显我们已经发现规律了。 只有简单地构造一下1234start_urls = ["http://www.allitebooks.com/"]for i in range(2, 781): start_urls.append("http://www.allitebooks.com/" + 'page/%d/' % i)print(start_urls) 获取首页10本书的链接采用BeautifulSoup 中select的 方法可以看出 书的链接 book_links 放在 h2 class = “entry-title” 下 。 返回的是所有书的链接，是一个list。1book_links = soup.select('.entry-title') 用select的方法取得 a 里面的东西,a 里面只有一个元素，[0]解开这个list 获得这个元素。然后选取[‘href’]这个属性获得电子书PDF的链接再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。12345for book_link in book_links : links.append(book_link.select('a')[0]['href']) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取['href']这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select('a')[0]['href'] print(link) #打印出来看看 2-获取书的分页123456789101112131415161718192021222324252627282930# 导入必要的模块import urllib.requestimport timeimport requestsimport urllib.requestfrom bs4 import BeautifulSoupfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETEDheaders = &#123; &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0&apos;,&#125;#url = &apos;http://www.allitebooks.com/&apos;download_links = [] #创建一个空列表来存储所有书的下载链接links = [] #创建一个空列表来存储所有书的链接# 传入函数： 网页的网址urldef get_book_link(url): # 获取网页的源代码 web_data = requests.get(url) time.sleep(1) #links = [] #建一个空的links来存放每一个页面书的链接，这样后面的就可以调用里面的url了 soup = BeautifulSoup(web_data.text, &apos;lxml&apos;) book_links = soup.select(&apos;.entry-title&apos;) #print(bodys) #返回的是一个列表放在H2里面，需要for循环取出元素 for book_link in book_links : links.append(book_link.select(&apos;a&apos;)[0][&apos;href&apos;]) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取[&apos;href&apos;]这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select(&apos;a&apos;)[0][&apos;href&apos;] print(link) #打印出来看看 3-获取书的下载链接 创建一个空的列表 download_links = [] ，来存储待会抓取的下载链接 定义一个获取下载函数。用同样的方法 取得 12345678download_links = [] #创建一个空列表来存储所有书的下载链接def get_download_link(url): web_data1 = requests.get(url) time.sleep(1) soup1 = BeautifulSoup(web_data1.text, 'lxml') downloadlink = soup1.select('.download-links')[0].select('a')[0]['href'] #返回的还是一个list，用select的方法取得 a 里面的东西,发现返回有两个link。，[0]解开这个list得到第一个link。然后选取['href']这个属性获得电子书PDF的链接 download_links.append(downloadlink) print(downloadlink) 4-下载。定义一个下载PDF的函数（通过二进制的方法写入数据） 原来作者用的是 。 实际运行起来会报错。1urllib.request.urlretrieve(url, book_name) 403 错误12raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 403: Forbidden 所以只能用二进制的方法写入数据了12345def download(url): book_name = url.split('/')[-1] #分割网址 获取书名 res = requests.get(url) with open(book_name, 'wb') as f: #二进制写入文件，保存PDF f.write(res.content) 先上单线程由于书的数量太大。现在只是下载前2页作为演示. 20本书妥妥的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# -*- coding:utf-8 -*-# 本爬虫用来下载http://www.allitebooks.com/中的电子书# 本爬虫将需要下载的书的链接写入txt文件，便于永久使用# 网站http://www.allitebooks.com/提供编程方面的电子书# 导入必要的模块import urllib.requestimport timeimport requestsimport urllib.requestfrom bs4 import BeautifulSoupfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETEDheaders = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0',&#125;#url = 'http://www.allitebooks.com/'links = [] #创建一个空列表来存储所有书的链接# 传入函数： 网页的网址urldef get_book_link(url): # 获取网页的源代码 web_data = requests.get(url) time.sleep(1) #links = [] #建一个空的links来存放每一个页面书的链接，这样后面的就可以调用里面的url了 soup = BeautifulSoup(web_data.text, 'lxml') book_links = soup.select('.entry-title') #print(bodys) #返回的是一个列表放在H2里面，需要for循环取出元素 for book_link in book_links : links.append(book_link.select('a')[0]['href']) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取['href']这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select('a')[0]['href'] print(link) #打印出来看看 #time.sleep(2)download_links = [] #创建一个空列表来存储所有书的下载链接def get_download_link(url): web_data1 = requests.get(url) time.sleep(1) soup1 = BeautifulSoup(web_data1.text, 'lxml') downloadlink = soup1.select('.download-links')[0].select('a')[0]['href'] #返回的还是一个list，用select的方法取得 a 里面的东西,发现返回有两个link。，[0]解开这个list得到第一个link。然后选取['href']这个属性获得电子书PDF的链接 download_links.append(downloadlink) print(downloadlink) #print(downloadlink[0].select('a')[0]['href']) #print(len(downloadlink)) #print(type(downloadlink[0]))def download(url): book_name = url.split('/')[-1] #分割网址 获取书名 res = requests.get(url) with open(book_name, 'wb') as f: #二进制写入文件，保存PDF f.write(res.content)if __name__=='__main__': start_urls = ["http://www.allitebooks.com/"] for i in range(2, 3): start_urls.append("http://www.allitebooks.com/" + 'page/%d/' % i) print(start_urls) for url in start_urls : get_book_link(url) print(links) print(len(links)) with open('book1.txt', 'w') as f: for book_link in links: f.writelines(book_link+'\n') print('写入txt文件完毕!') for url in links: get_download_link(url) print('获取下载链接') print(url) # 统计该爬虫的消耗时间 t1 = time.time() print('*' * 50) for url in download_links: print(url) print('开始下载') download(url) print('下载完成') t2 = time.time() print('不使用多线程，总共耗时：%s' % (t2 - t1)) print('*' * 50) 多单线程由于书的数量太大。现在只是下载前2页作为演示. 20本书妥妥的12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# -*- coding:utf-8 -*-# 本爬虫用来下载http://www.allitebooks.com/中的电子书# 本爬虫将需要下载的书的链接写入txt文件，便于永久使用# 网站http://www.allitebooks.com/提供编程方面的电子书# 导入必要的模块import urllib.requestimport timeimport requestsimport urllib.requestfrom bs4 import BeautifulSoupfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETEDheaders = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0',&#125;#url = 'http://www.allitebooks.com/'links = [] #创建一个空列表来存储所有书的链接# 传入函数： 网页的网址urldef get_book_link(url): # 获取网页的源代码 web_data = requests.get(url) time.sleep(1) #links = [] #建一个空的links来存放每一个页面书的链接，这样后面的就可以调用里面的url了 soup = BeautifulSoup(web_data.text, 'lxml') book_links = soup.select('.entry-title') #print(bodys) #返回的是一个列表放在H2里面，需要for循环取出元素 for book_link in book_links : links.append(book_link.select('a')[0]['href']) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取['href']这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select('a')[0]['href'] print(link) #打印出来看看 #time.sleep(2)download_links = [] #创建一个空列表来存储所有书的下载链接def get_download_link(url): web_data1 = requests.get(url) time.sleep(1) soup1 = BeautifulSoup(web_data1.text, 'lxml') downloadlink = soup1.select('.download-links')[0].select('a')[0]['href'] #返回的还是一个list，用select的方法取得 a 里面的东西,发现返回有两个link。，[0]解开这个list得到第一个link。然后选取['href']这个属性获得电子书PDF的链接 download_links.append(downloadlink) print(downloadlink) #print(downloadlink[0].select('a')[0]['href']) #print(len(downloadlink)) #print(type(downloadlink[0]))def download(url): book_name = url.split('/')[-1] #分割网址 获取书名 res = requests.get(url) with open(book_name, 'wb') as f: #二进制写入文件，保存PDF f.write(res.content)if __name__=='__main__': start_urls = ["http://www.allitebooks.com/"] for i in range(2, 3): start_urls.append("http://www.allitebooks.com/" + 'page/%d/' % i) print(start_urls) for url in start_urls : get_book_link(url) print(links) print(len(links)) with open('book1.txt', 'w') as f: for book_link in links: f.writelines(book_link+'\n') print('写入txt文件完毕!') for url in links: get_download_link(url) print('获取下载链接') print(url) ####下面是多线程代码######## print('*' * 50) t3 = time.time() # 利用并发下载电影图片 executor = ThreadPoolExecutor(max_workers=3) # 可以自己调整max_workers,即线程的个数 # submit()的参数： 第一个为函数， 之后为该函数的传入参数，允许有多个 future_tasks = [executor.submit(download, url) for url in download_links] # 等待所有的线程完成，才进入后续的执行 wait(future_tasks, return_when=ALL_COMPLETED) t4 = time.time() print('使用多线程，总共耗时：%s' % (t4 - t3)) print('*' * 50) 省一半时间 自此python 单线程、多线程爬虫完毕。希望大家继续关注我。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【python3爬虫】利用python的单线程和多线程下载PDF电子书]]></title>
    <url>%2F2018%2F12%2F15%2F%E3%80%90python3%E7%88%AC%E8%99%AB%E3%80%91%E5%88%A9%E7%94%A8python%E7%9A%84%E5%8D%95%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8B%E8%BD%BDPDF%E7%94%B5%E5%AD%90%E4%B9%A6-1%2F</url>
    <content type="text"><![CDATA[环境配置：python3.6 ；win7 本文是参考了https://mp.weixin.qq.com/s/cQRG7Ix0nP4MptlaMqSWvA 。但是发现不能正确运行。所以笔者打算救活这只爬虫。1http://www.allitebooks.com/ 该网站提供了大量免费的编程方面的电子书，可以供大家练手。 爬虫思路1-获取书的页数2-获取书的分页3-获取书的下载链接4-下载5-加入多线程 1-获取书的页数 ，分析网址构造首页 ： http://www.allitebooks.com/第二页：http://www.allitebooks.com/page/2/第三页： http://www.allitebooks.com/page/3/最后一页：http://www.allitebooks.com/page/780/好明显我们已经发现规律了。 只有简单地构造一下1234start_urls = ["http://www.allitebooks.com/"]for i in range(2, 781): start_urls.append("http://www.allitebooks.com/" + 'page/%d/' % i)print(start_urls) 获取首页10本书的链接采用BeautifulSoup 中select的 方法可以看出 书的链接 book_links 放在 h2 class = “entry-title” 下 。 返回的是所有书的链接，是一个list。1book_links = soup.select('.entry-title') 用select的方法取得 a 里面的东西,a 里面只有一个元素，[0]解开这个list 获得这个元素。然后选取[‘href’]这个属性获得电子书PDF的链接再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。12345for book_link in book_links : links.append(book_link.select('a')[0]['href']) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取['href']这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select('a')[0]['href'] print(link) #打印出来看看 2-获取书的分页123456789101112131415161718192021222324252627282930# 导入必要的模块import urllib.requestimport timeimport requestsimport urllib.requestfrom bs4 import BeautifulSoupfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETEDheaders = &#123; &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0&apos;,&#125;#url = &apos;http://www.allitebooks.com/&apos;download_links = [] #创建一个空列表来存储所有书的下载链接links = [] #创建一个空列表来存储所有书的链接# 传入函数： 网页的网址urldef get_book_link(url): # 获取网页的源代码 web_data = requests.get(url) time.sleep(1) #links = [] #建一个空的links来存放每一个页面书的链接，这样后面的就可以调用里面的url了 soup = BeautifulSoup(web_data.text, &apos;lxml&apos;) book_links = soup.select(&apos;.entry-title&apos;) #print(bodys) #返回的是一个列表放在H2里面，需要for循环取出元素 for book_link in book_links : links.append(book_link.select(&apos;a&apos;)[0][&apos;href&apos;]) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取[&apos;href&apos;]这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select(&apos;a&apos;)[0][&apos;href&apos;] print(link) #打印出来看看 3-获取书的下载链接 创建一个空的列表 download_links = [] ，来存储待会抓取的下载链接 定义一个获取下载函数。用同样的方法 取得 12345678download_links = [] #创建一个空列表来存储所有书的下载链接def get_download_link(url): web_data1 = requests.get(url) time.sleep(1) soup1 = BeautifulSoup(web_data1.text, 'lxml') downloadlink = soup1.select('.download-links')[0].select('a')[0]['href'] #返回的还是一个list，用select的方法取得 a 里面的东西,发现返回有两个link。，[0]解开这个list得到第一个link。然后选取['href']这个属性获得电子书PDF的链接 download_links.append(downloadlink) print(downloadlink) 4-下载。定义一个下载PDF的函数（通过二进制的方法写入数据） 原来作者用的是 。 实际运行起来会报错。1urllib.request.urlretrieve(url, book_name) 403 错误12raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 403: Forbidden 所以只能用二进制的方法写入数据了12345def download(url): book_name = url.split('/')[-1] #分割网址 获取书名 res = requests.get(url) with open(book_name, 'wb') as f: #二进制写入文件，保存PDF f.write(res.content) 先上单线程由于书的数量太大。现在只是下载前2页作为演示. 20本书妥妥的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# -*- coding:utf-8 -*-# 本爬虫用来下载http://www.allitebooks.com/中的电子书# 本爬虫将需要下载的书的链接写入txt文件，便于永久使用# 网站http://www.allitebooks.com/提供编程方面的电子书# 导入必要的模块import urllib.requestimport timeimport requestsimport urllib.requestfrom bs4 import BeautifulSoupfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETEDheaders = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0',&#125;#url = 'http://www.allitebooks.com/'links = [] #创建一个空列表来存储所有书的链接# 传入函数： 网页的网址urldef get_book_link(url): # 获取网页的源代码 web_data = requests.get(url) time.sleep(1) #links = [] #建一个空的links来存放每一个页面书的链接，这样后面的就可以调用里面的url了 soup = BeautifulSoup(web_data.text, 'lxml') book_links = soup.select('.entry-title') #print(bodys) #返回的是一个列表放在H2里面，需要for循环取出元素 for book_link in book_links : links.append(book_link.select('a')[0]['href']) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取['href']这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select('a')[0]['href'] print(link) #打印出来看看 #time.sleep(2)download_links = [] #创建一个空列表来存储所有书的下载链接def get_download_link(url): web_data1 = requests.get(url) time.sleep(1) soup1 = BeautifulSoup(web_data1.text, 'lxml') downloadlink = soup1.select('.download-links')[0].select('a')[0]['href'] #返回的还是一个list，用select的方法取得 a 里面的东西,发现返回有两个link。，[0]解开这个list得到第一个link。然后选取['href']这个属性获得电子书PDF的链接 download_links.append(downloadlink) print(downloadlink) #print(downloadlink[0].select('a')[0]['href']) #print(len(downloadlink)) #print(type(downloadlink[0]))def download(url): book_name = url.split('/')[-1] #分割网址 获取书名 res = requests.get(url) with open(book_name, 'wb') as f: #二进制写入文件，保存PDF f.write(res.content)if __name__=='__main__': start_urls = ["http://www.allitebooks.com/"] for i in range(2, 3): start_urls.append("http://www.allitebooks.com/" + 'page/%d/' % i) print(start_urls) for url in start_urls : get_book_link(url) print(links) print(len(links)) with open('book1.txt', 'w') as f: for book_link in links: f.writelines(book_link+'\n') print('写入txt文件完毕!') for url in links: get_download_link(url) print('获取下载链接') print(url) # 统计该爬虫的消耗时间 t1 = time.time() print('*' * 50) for url in download_links: print(url) print('开始下载') download(url) print('下载完成') t2 = time.time() print('不使用多线程，总共耗时：%s' % (t2 - t1)) print('*' * 50) 多单线程由于书的数量太大。现在只是下载前2页作为演示. 20本书妥妥的12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# -*- coding:utf-8 -*-# 本爬虫用来下载http://www.allitebooks.com/中的电子书# 本爬虫将需要下载的书的链接写入txt文件，便于永久使用# 网站http://www.allitebooks.com/提供编程方面的电子书# 导入必要的模块import urllib.requestimport timeimport requestsimport urllib.requestfrom bs4 import BeautifulSoupfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETEDheaders = &#123; 'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0',&#125;#url = 'http://www.allitebooks.com/'links = [] #创建一个空列表来存储所有书的链接# 传入函数： 网页的网址urldef get_book_link(url): # 获取网页的源代码 web_data = requests.get(url) time.sleep(1) #links = [] #建一个空的links来存放每一个页面书的链接，这样后面的就可以调用里面的url了 soup = BeautifulSoup(web_data.text, 'lxml') book_links = soup.select('.entry-title') #print(bodys) #返回的是一个列表放在H2里面，需要for循环取出元素 for book_link in book_links : links.append(book_link.select('a')[0]['href']) #返回的还是一个list，用select的方法取得 a 里面的东西，[0]解开这个list。然后选取['href']这个属性获得电子书PDF的链接 #再用append的方式，一个一个地把链接追加到上面构造的link[]里面去。 link = book_link.select('a')[0]['href'] print(link) #打印出来看看 #time.sleep(2)download_links = [] #创建一个空列表来存储所有书的下载链接def get_download_link(url): web_data1 = requests.get(url) time.sleep(1) soup1 = BeautifulSoup(web_data1.text, 'lxml') downloadlink = soup1.select('.download-links')[0].select('a')[0]['href'] #返回的还是一个list，用select的方法取得 a 里面的东西,发现返回有两个link。，[0]解开这个list得到第一个link。然后选取['href']这个属性获得电子书PDF的链接 download_links.append(downloadlink) print(downloadlink) #print(downloadlink[0].select('a')[0]['href']) #print(len(downloadlink)) #print(type(downloadlink[0]))def download(url): book_name = url.split('/')[-1] #分割网址 获取书名 res = requests.get(url) with open(book_name, 'wb') as f: #二进制写入文件，保存PDF f.write(res.content)if __name__=='__main__': start_urls = ["http://www.allitebooks.com/"] for i in range(2, 3): start_urls.append("http://www.allitebooks.com/" + 'page/%d/' % i) print(start_urls) for url in start_urls : get_book_link(url) print(links) print(len(links)) with open('book1.txt', 'w') as f: for book_link in links: f.writelines(book_link+'\n') print('写入txt文件完毕!') for url in links: get_download_link(url) print('获取下载链接') print(url) ####下面是多线程代码######## print('*' * 50) t3 = time.time() # 利用并发下载电影图片 executor = ThreadPoolExecutor(max_workers=3) # 可以自己调整max_workers,即线程的个数 # submit()的参数： 第一个为函数， 之后为该函数的传入参数，允许有多个 future_tasks = [executor.submit(download, url) for url in download_links] # 等待所有的线程完成，才进入后续的执行 wait(future_tasks, return_when=ALL_COMPLETED) t4 = time.time() print('使用多线程，总共耗时：%s' % (t4 - t3)) print('*' * 50) 省一半时间 自此python 单线程、多线程爬虫完毕。希望大家继续关注我。]]></content>
  </entry>
</search>
